from langchain_chroma import Chroma
from langchain_ollama import OllamaEmbeddings
from langchain_openai import OpenAIEmbeddings
from langchain.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_ollama import ChatOllama
from langchain_openai import ChatOpenAI
from langchain_core.retrievers import BaseRetriever
from langchain_core.documents import Document
from config import (
    DB_DIR, NOMBRE_COLECCION,
    USAR_OLLAMA, MODELO_OLLAMA, OPENAI_API_KEY, MODELO_OPENAI
)
import re
from typing import List

# Plantilla de prompt del asistente acad√©mico
PROMPT_ACADEMICO = """Eres un asistente acad√©mico experto. Tu √öNICA fuente de informaci√≥n es el contexto de s√≠labos proporcionado abajo.

INSTRUCCIONES CR√çTICAS:
1. LEE CUIDADOSAMENTE el contexto (secciones de s√≠labos reales)
2. BUSCA informaci√≥n sobre el curso/tema solicitado
3. EXTRAE directamente del contexto:
   - Nombre exacto del curso
   - Ciclo acad√©mico (nivel b√°sico, intermedio, avanzado)
   - Datos generales (cr√©ditos, horas, prerequisitos)
   - Descripci√≥n y objetivos del curso
   - Resultados de aprendizaje
   - Metodolog√≠a
   - Sistema de evaluaci√≥n y f√≥rmula del promedio final
   - Contenido programado por semanas
   - Bibliograf√≠a (b√°sica y complementaria)

4. Si el usuario pregunta por un curso ESPEC√çFICO:
   - Busca el nombre del curso en el contexto
   - Extrae TODOS los detalles disponibles
   - Cita la fuente (nombre del s√≠labo)

5. RESPONDE SIEMPRE bas√°ndote en el contexto:
   - NO inventes informaci√≥n
   - SI no est√° en el contexto, dilo expl√≠citamente
   - CITA las fuentes (nombre del s√≠labo de origen)

6. Formato de respuesta:
   - Claro y estructurado
   - Usa secciones (Datos generales, Evaluaci√≥n, Contenido, etc.)
   - Incluye la fuente (s√≠labo)

CONTEXTO DE LOS S√çLABOS (informaci√≥n verificada):
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
{context}
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

PREGUNTA DEL USUARIO:
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
{question}
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

RESPUESTA (basada √öNICAMENTE en el contexto anterior):
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ"""

class RecuperadorHibrido(BaseRetriever):
    """Recuperador h√≠brido que combina b√∫squeda sem√°ntica con coincidencia de palabras clave.

    Mejora la recuperaci√≥n priorizando chunks que contienen el nombre exacto del curso
    buscado, combinado con la similitud sem√°ntica.
    """

    vectorstore_retriever: BaseRetriever
    vectorstore: Chroma

    class Config:
        arbitrary_types_allowed = True

    def _extract_keywords(self, text: str) -> List[str]:
        """Extrae palabras clave significativas del texto"""
        # Convertir a min√∫sculas
        text = text.lower()
        # Eliminar caracteres especiales pero mantener espacios
        text = re.sub(r'[^a-z√°√©√≠√≥√∫√±\s]', ' ', text)
        # Dividir en palabras
        palabras = text.split()
        # Filtrar palabras cortas (menos de 3 caracteres)
        return [p for p in palabras if len(p) >= 3]

    def _score_keyword_match(self, chunk_content: str, keywords: List[str]) -> tuple:
        """Calcula puntuaci√≥n de coincidencia de palabras clave y detecta nombres de cursos.

        Retorna (puntuaci√≥n_general, tiene_nombre_curso_especifico)
        """
        if not keywords:
            return (0.0, False)

        chunk_lower = chunk_content.lower()
        matches = sum(1 for kw in keywords if kw in chunk_lower)

        # Detectar si hay m√∫ltiples palabras clave consecutivas (posible nombre de curso)
        # Ej: "investigaci√≥n operativa" tiene ambas palabras
        keywords_muy_especificos = ["investigaci√≥n", "operativa", "√©tica", "deontolog√≠a",
                                     "redes", "comunicaciones", "gesti√≥n", "calidad"]

        tiene_nombre_curso = any(
            kw in chunk_lower for kw in keywords
            if kw in keywords_muy_especificos
        )

        score = matches / len(keywords) if len(keywords) > 0 else 0.0
        return (score, tiene_nombre_curso)

    def _get_relevant_docs(self, query: str, k: int = 10) -> List[Document]:
        """Recupera y re-ordena documentos combinando sem√°ntica con palabras clave"""
        # Obtener documentos por similitud sem√°ntica (k+5 para tener m√°s opciones)
        docs_semanticos = self.vectorstore_retriever.invoke(query)

        # Extraer palabras clave de la consulta
        keywords = self._extract_keywords(query)

        # Si no hay palabras clave significativas, devolver resultados sem√°nticos
        if not keywords:
            return docs_semanticos[:k]

        # Re-ordenar documentos priorizando:
        # 1. Coincidencia de nombres de cursos espec√≠ficos (boost alto)
        # 2. Similitud sem√°ntica
        # 3. Coincidencia de palabras clave generales
        docs_scored = []
        for i, doc in enumerate(docs_semanticos):
            keyword_score, tiene_nombre = self._score_keyword_match(doc.page_content, keywords)

            # Scoring estrat√©gico:
            # - Si tiene nombre de curso espec√≠fico: boost muy alto
            # - Si no, usar posici√≥n sem√°ntica como base
            if tiene_nombre:
                # Nombre de curso encontrado: dar m√°xima prioridad
                score = 1000.0 + keyword_score  # Boost masivo para nombres de curso
            else:
                # Puntuaci√≥n basada en posici√≥n sem√°ntica + palabras clave
                score = (1.0 / (i + 1)) + (keyword_score * 0.5)

            docs_scored.append((doc, score))

        # Ordenar por puntuaci√≥n combinada
        docs_scored.sort(key=lambda x: x[1], reverse=True)

        # Retornar solo los documentos (sin las puntuaciones)
        return [doc for doc, _ in docs_scored[:k]]

    async def _aget_relevant_documents(self, query: str) -> List[Document]:
        """Versi√≥n as√≠ncrona de obtener documentos relevantes"""
        return self._get_relevant_docs(query)

    def _get_relevant_documents(self, query: str) -> List[Document]:
        """Obtiene documentos relevantes usando b√∫squeda h√≠brida"""
        return self._get_relevant_docs(query)

def obtener_embeddings():
    """Obtiene el modelo de embeddings seg√∫n la configuraci√≥n"""
    if USAR_OLLAMA:
        return OllamaEmbeddings(model=MODELO_OLLAMA)
    else:
        if not OPENAI_API_KEY:
            raise ValueError("OPENAI_API_KEY no configurada. Config√∫rala en .env o usa Ollama")
        return OpenAIEmbeddings(api_key=OPENAI_API_KEY)

def inicializar_sistema_rag():
    """Inicializa el sistema RAG con Chroma y LLM"""

    # Cargar embeddings
    print("Cargando embeddings...")
    embeddings = obtener_embeddings()

    # Cargar base de datos Chroma
    print("Cargando base de datos Chroma...")
    vectorstore = Chroma(
        collection_name=NOMBRE_COLECCION,
        embedding_function=embeddings,
        persist_directory=str(DB_DIR)
    )

    # Crear recuperador sem√°ntico base
    recuperador_semantico = vectorstore.as_retriever(
        search_type="similarity",
        search_kwargs={"k": 15}  # Aumentado a 15 para dar m√°s opciones al recuperador h√≠brido
    )

    # Crear recuperador h√≠brido que combina sem√°ntica + palabras clave
    print("Usando recuperador h√≠brido (sem√°ntica + palabras clave)...")
    recuperador = RecuperadorHibrido(
        vectorstore_retriever=recuperador_semantico,
        vectorstore=vectorstore
    )

    # Inicializar LLM
    print("Inicializando modelo de lenguaje...")
    if USAR_OLLAMA:
        llm = ChatOllama(model=MODELO_OLLAMA, temperature=0.3)
        print(f"Usando Ollama con modelo: {MODELO_OLLAMA}")
    else:
        if not OPENAI_API_KEY:
            raise ValueError("OPENAI_API_KEY no configurada. Config√∫rala en .env o usa Ollama")
        llm = ChatOpenAI(model=MODELO_OPENAI, temperature=0.3, api_key=OPENAI_API_KEY)
        print(f"Usando OpenAI con modelo: {MODELO_OPENAI}")

    # Crear plantilla de prompt
    prompt = PromptTemplate(
        template=PROMPT_ACADEMICO,
        input_variables=["context", "question"]
    )

    # Crear cadena RAG
    cadena_rag = (
        {
            "context": recuperador | (lambda docs: "\n\n".join([doc.page_content for doc in docs])),
            "question": lambda x: x
        }
        | prompt
        | llm
        | StrOutputParser()
    )

    return cadena_rag, recuperador

def consultar_rag(cadena_rag, pregunta: str, recuperador=None):
    """Consulta el sistema RAG y muestra los chunks utilizados"""
    print(f"\nüìö Pregunta: {pregunta}\n")

    # Mostrar chunks recuperados si se proporciona el recuperador
    if recuperador:
        print("=" * 80)
        print("üìñ CHUNKS RECUPERADOS DE LA BASE DE DATOS")
        print("=" * 80)

        documentos_recuperados = recuperador.invoke(pregunta)

        for i, doc in enumerate(documentos_recuperados, 1):
            # Obtener informaci√≥n del documento
            fuente = doc.metadata.get('source', 'Desconocida')
            nombre_archivo = fuente.split('/')[-1] if '/' in fuente else fuente

            print(f"\n[CHUNK {i}] Fuente: {nombre_archivo}")
            print("-" * 80)
            print(f"Contenido:\n{doc.page_content}")
            print("-" * 80)

        print("\n" + "=" * 80)
        print("ü§ñ RESPUESTA GENERADA")
        print("=" * 80 + "\n")

    # Generar respuesta
    respuesta = cadena_rag.invoke(pregunta)
    print(f"{respuesta}\n")

    return respuesta
