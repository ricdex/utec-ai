# ğŸ“ Asistente AcadÃ©mico RAG

Sistema RAG (Retrieval-Augmented Generation) en Python para consultar sÃ­labos acadÃ©micos usando Chroma como base de datos vectorial.

## Inicio RÃ¡pido

```bash
# 1. Instalar dependencias
make install

# 2. Elegir proveedor de LLM
# OpciÃ³n A: Ollama (local, recomendado, sin API key)
ollama pull llama2
ollama serve  # En otra terminal

# OpciÃ³n B: OpenAI API
export OPENAI_API_KEY="sk-..."

# 3. Ejecutar
make run
```

## InstalaciÃ³n Detallada

### Paso 1: Instalar dependencias
```bash
make install
```

### Paso 2: Configurar el modelo de lenguaje

**OpciÃ³n A: Ollama (Recomendado - Local, sin costo)**
```bash
# 1. Descargar e instalar desde https://ollama.ai
# 2. En terminal 1, ejecutar el servidor:
ollama serve

# 3. En terminal 2, descargar el modelo:
ollama pull llama2
```

**OpciÃ³n B: OpenAI API**
```bash
# Exportar tu API key de OpenAI
export OPENAI_API_KEY="sk-..."

# O crear un archivo .env:
echo "OPENAI_API_KEY=sk-..." > .env
```

## Uso

### Modo Interactivo (Recomendado)
```bash
make run
```

Luego escribe preguntas como:
- "Â¿QuÃ© cursos enseÃ±an gestiÃ³n de proyectos?"
- "Â¿En quÃ© ciclo se aborda Ã©tica profesional?"
- "Â¿CuÃ¡l es la fÃ³rmula del promedio final?"

**Cada respuesta muestra automÃ¡ticamente los chunks recuperados** para que verifiques que se estÃ¡ usando la informaciÃ³n correcta.

### Desde cÃ³digo Python
```python
from rag_system import inicializar_sistema_rag, consultar_rag

cadena_rag, recuperador = inicializar_sistema_rag()

# Mostrar chunks recuperados
consultar_rag(cadena_rag, "Â¿QuÃ© cursos enseÃ±an gestiÃ³n de proyectos?", recuperador)

# Sin mostrar chunks (mÃ¡s silencioso)
consultar_rag(cadena_rag, "Tu pregunta aquÃ­")
```

## Estructura de archivos

```
.
â”œâ”€â”€ config.py              # ConfiguraciÃ³n centralizada
â”œâ”€â”€ load_documents.py      # Carga PDFs de sÃ­labos en Chroma
â”œâ”€â”€ rag_system.py          # Sistema RAG con bÃºsqueda semÃ¡ntica
â”œâ”€â”€ main.py               # Interfaz interactiva
â”œâ”€â”€ requirements.txt      # Dependencias Python
â”œâ”€â”€ Makefile             # Comandos Ãºtiles
â”œâ”€â”€ .env.example         # Plantilla para variables de entorno
â”œâ”€â”€ silabus/             # PDFs de sÃ­labos de la carrera
â””â”€â”€ chroma_db/          # BD vectorial Chroma (creada automÃ¡ticamente)
```

## CaracterÃ­sticas

âœ¨ **Minimalista**: Solo ~150 lÃ­neas de cÃ³digo Python esencial
ğŸ“š **RAG AutomÃ¡tico**: RecuperaciÃ³n por similitud semÃ¡ntica + generaciÃ³n
ğŸ—‚ï¸ **Chroma**: BD vectorial eficiente para bÃºsqueda semÃ¡ntica
ğŸ¤– **Flexible**: Usa Ollama (local) o OpenAI API
ğŸ” **BÃºsqueda Inteligente**: Encuentra cursos relacionados por temas
ğŸ“– **VisualizaciÃ³n de Chunks**: Ve exactamente quÃ© informaciÃ³n usa para responder
ğŸ”§ **Debugging Integrado**: Verifica si se estÃ¡n usando los datos correctos

## CÃ³mo Funciona

1. **IndexaciÃ³n**: Los PDFs se dividen en fragmentos y se convierten a vectores
2. **Consulta**: El usuario pregunta sobre un tema
3. **BÃºsqueda**: Se buscan los 5 fragmentos mÃ¡s similares
4. **GeneraciÃ³n**: El LLM responde con informaciÃ³n estructurada:
   - Cursos que cubren el tema
   - Ciclo acadÃ©mico
   - BibliografÃ­a
   - Nivel de dificultad

## PersonalizaciÃ³n

### Cambiar el prompt del asistente
Edita `rag_system.py`, variable `PROMPT_ACADEMICO`

### Cambiar modelo de embeddings
En `config.py`, modifica `MODELO_EMBEDDINGS`

### Ajustar tamaÃ±o de fragmentos
En `config.py`, modifica `TAMAÃ‘O_CHUNK` y `OVERLAP_CHUNK`

## SoluciÃ³n de Problemas

**Error: "No se puede conectar a Ollama"**
```bash
# Verifica que el servidor estÃ¡ ejecutÃ¡ndose:
ollama serve
```

**Error: "OPENAI_API_KEY no configurada"**
```bash
# Configura tu clave:
export OPENAI_API_KEY="sk-..."
# Luego cambia USAR_OLLAMA = False en config.py
```

**La BD Chroma tarda mucho en crearse la primera vez**
- Esto es normal. Espera a que se complete.
- Las siguientes ejecuciones serÃ¡n mÃ¡s rÃ¡pidas.

## Requisitos

- Python 3.9+
- 2-4 GB de RAM disponible
- Ollama instalado (si usas Ollama)
- O API key de OpenAI (si usas OpenAI)
